---
layout:     post   				    # 使用的布局
title:      13.0 隐马尔科夫模型 				# 标题 
date:       2018-08-16 				# 时间
author:     子颢 						# 作者
catalog: true 						# 是否归档
tags:								#标签
    - 机器学习
    - HMM
    - 隐马尔科夫模型
---

# 算法原理

概率图模型是一类用图来表达变量之间相关关系的概率模型。马尔科夫网络假设随机过程中各个状态St的概率分布，只与他的前一个状态有关，即P(St|S1,S2,...,St-1) = P(St|St-1)，所以可以认为马尔科夫网络是一种特殊的贝叶斯网络。
隐马尔科夫模型是含有隐变量的马尔科夫网络，该模型包含两种类型的变量：一种是观测变量{x1,x2,...,xn}，表示第i时刻的观测值；一种是状态变量{y1,y2,...,yn}，即隐变量，表示第i时刻的状态。
![HMM](/img/HMM-01.png)
所以所有变量的联合概率分布为：
![HMM](/img/HMM-02.png)
要确定一个隐马尔科夫模型，需要三组参数：























# 模型训练

代码地址 <a href="https://github.com/qianshuang/ml-exp" target="_blank">https://github.com/qianshuang/ml-exp</a>

```
def train():
    print("start training...")
    # 处理训练数据
    # train_feature, train_target = process_file(train_dir, word_to_id, cat_to_id)  # 词频特征
    train_data = process_maxent_file(train_dir, word_to_id, cat_to_id)  # 最大熵词频特征
    # train_feature, train_target = process_tfidf_file(train_dir, word_to_id, cat_to_id)  # TF-IDF特征
    # 模型训练
    # model.fit(train_feature, train_target)
    model.train(train_data)


def test():
    print("start testing...")
    # 处理测试数据
    # test_feature, test_target = process_file(test_dir, word_to_id, cat_to_id)
    test_data = process_maxent_file(test_dir, word_to_id, cat_to_id)  # 最大熵词频特征
    # test_feature, test_target = process_tfidf_file(test_dir, word_to_id, cat_to_id)  # 不能直接这样处理，应该取训练集的IDF值
    # test_predict = model.predict(test_feature)  # 返回预测类别
    # test_predict_proba = model.predict_proba(test_feature)    # 返回属于各个类别的概率
    # test_predict = np.argmax(test_predict_proba, 1)  # 返回概率最大的类别标签

    # MaxEnt测试
    wordid_freq_jsons = []
    test_target = []
    for i in range(len(test_data)):
        wordid_freq, label_id = test_data[i]
        test_target.append(label_id)
        wordid_freq_jsons.append(json.dumps(wordid_freq))

    test_predict = model.prob_classify_many(wordid_freq_jsons)

    # accuracy
    true_false = (test_predict == test_target)
    accuracy = np.count_nonzero(true_false) / float(len(test_target))
    print()
    print("accuracy is %f" % accuracy)

    # precision    recall  f1-score
    print()
    print(metrics.classification_report(test_target, test_predict, target_names=categories))

    # 混淆矩阵
    print("Confusion Matrix...")
    print(metrics.confusion_matrix(test_target, test_predict))


if not os.path.exists(vocab_dir):
    # 构建词典表
    build_vocab(train_dir, vocab_dir)

categories, cat_to_id = read_category()
words, word_to_id = read_vocab(vocab_dir)

# kNN
# model = neighbors.KNeighborsClassifier()
# decision tree
# model = tree.DecisionTreeClassifier()
# random forest
# model = ensemble.RandomForestClassifier(n_estimators=10)  # n_estimators为基决策树的数量，一般越大效果越好直至趋于收敛
# AdaBoost
# model = ensemble.AdaBoostClassifier(learning_rate=1.0)  # learning_rate的作用是收缩基学习器的权重贡献值
# GBDT
# model = ensemble.GradientBoostingClassifier(n_estimators=10)
# xgboost
# model = xgboost.XGBClassifier(n_estimators=10)
# Naive Bayes
# model = naive_bayes.MultinomialNB()
# logistic regression
# model = linear_model.LogisticRegression()   # ovr
# model = linear_model.LogisticRegression(multi_class="multinomial", solver="lbfgs")  # softmax回归
# SVM
# model = svm.LinearSVC()  # 线性，无概率结果
# model = svm.SVC(probability=True)  # 核函数，训练慢
# MaxEnt
model = nltk.classify.MaxentClassifier


train()
test()
```
运行结果：
```
read_category...
read_vocab...
start training...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -2.30259        0.100
·············
·············
```

# 社群

- QQ交流群
	![562929489](/img/qq_ewm.png)
- 微信交流群
	![562929489](/img/wx_ewm.png)
- 微信公众号
	![562929489](/img/wxgzh_ewm.png)