---
layout:     post   				    # 使用的布局
title:      49.0 BERT			# 标题 
date:       2018-12-17  			# 时间
author:     子颢 						# 作者
catalog: true 						# 是否归档
tags:								# 标签
    - NLP
    - BERT
---

BERT（Bidirectional Encoder Representations from Transformers），是一种新型的语言模型。它通过联合调节所有层中的双向Transformer来训练预训练深度双向表示，并且只需要一个额外的输出层来对预训练BERT进行微调就可以满足各种任务（如SQuAD问答任务、命名实体识别以及情感识别等），没有必要针对特定任务对模型进行修改。下面将从BERT模型的结构、输入以及训练三个方面依次进行介绍。

# 模型结构

BERT是一种基于微调的多层双向Transformer编码器，其中的Transformer与原始的Transformer是相同的，并且实现了两个版本的BERT模型，在两个版本中前馈大小都设置为4层，其中层数（即Transformer blocks块）表示为L，隐藏层神经元大小表示为H，自注意力的数量为A。
1. BERT BASE：L=12，H=768，A=12，Total Parameters=110M
2. BERT LARGE：L=24，H=1024，A=16，Total Parameters=340M

# 模型输入

输入表示可以是单个文本（text）或一对文本（QA pair）。对于给定的词，其输入表示通过三部分Embedding求和组成：
![BERT](/img/BERT-01.jpg)
其中：
1. Token Embeddings表示的是词向量，第一个单词是CLS标志，可以用于之后的分类任务，对于非分类任务，可以忽略词向量。
2. Segment Embeddings用来区别两种句子，因为预训练不只做语言模型还要做以两个句子为输入的match任务。
3. Position Embeddings是通过模型学习得到的位置embedding。

# 模型训练

BERT模型使用两个新的有监督预测任务对BERT进行预训练，分别是MLM(Masked Language Model)和Next Sentence Prediction。

Masked LM为了训练深度双向Transformer表示，采用了一种简单的方法：随机掩盖部分输入词，然后对那些被掩盖的词进行预测。预训练的目标是构建语言模型，BERT模型采用的是bidirectional Transformer。那么为什么采用“bidirectional”的方式呢？因为在预训练语言模型来处理下游任务时，我们需要的不仅仅是某个词左侧的语言信息，还需要右侧的语言信息。在训练的过程中，随机地掩盖每个序列中15％的token，并不是像word2vec中的cbow那样去对每一个词都进行预测。

Next Sentence Prediction：很多句子级别的任务如自动问答（QA）和自然语言推理（NLI）都需要理解两个句子之间的关系。那么在这一任务中我们需要随机将数据划分为等大小的两部分，一部分数据中的两个语句对是上下文连续的，另一部分数据中的两个语句对是上下文不连续的。然后让Transformer模型来识别这些语句对中，哪些语句对是连续的，哪些对子不连续。




# 社群

- 微信公众号
	![562929489](/img/wxgzh_ewm.png)