---
layout:     post   				    # 使用的布局
title:      29.0 seq2seq 				# 标题 
date:       2018-09-01 				# 时间
author:     子颢 						# 作者
catalog: true 						# 是否归档
tags:								#标签
    - 深度学习
    - seq2seq
    - 机器翻译
---

# 算法原理

前面我们说到，机器翻译是delayed seq to seq，又叫Encoder–Decoder。Encoder模块将原始输入sequence压缩成一个“有意义”的向量表示，Decoder模块将该向量解码为目标输出sequence。
![S2S](/img/S2S-01.png)
![S2S](/img/S2S-02.png)
下面以一个实际例子来给大家详细介绍Encoder–Decoder模型的基本原理。该示例的目的是输入一个字符序列，输出这个字符序列的字典正序列，比如：输入“bsaqq”，输出“abqqs”。

首先我们来看下encoder模块，是一个普通的多层RNN。
```
# 1. encoder
encoder_cell = get_multi_rnn_cell(self.config.rnn_size, self.config.num_layers)
source_embedding = tf.get_variable('source_embedding', [self.config.source_vocab_size, self.config.encoding_embedding_size])
source_embedding_inputs = tf.nn.embedding_lookup(source_embedding, self.source)
encoder_output, encoder_state = tf.nn.dynamic_rnn(
    encoder_cell,
    source_embedding_inputs,
    sequence_length=self.source_sequence_length,
    dtype=tf.float32)
```

然后是decoder模块，decoder本质上也是一个RNN，它使用encoder模块的输出状态来初始化states，并在每个cell的输出添加full connected wrapper，以得到每个目标翻译的概率（score）。
```
# cut掉最后的字符
ending = tf.strided_slice(self.target, [0, 0], [self.config.batch_size, -1], [1, 1])
# 最前面加上<GO>字符
decoder_input = tf.concat([tf.fill([tf.shape(self.target)[0], 1], self.config.target_letter_to_id['<GO>']), ending], 1)
target_embedding = tf.get_variable('target_embedding', [self.config.target_vocab_size, self.config.decoding_embedding_size])
target_embedding_inputs = tf.nn.embedding_lookup(target_embedding, decoder_input)

# decoder_cell可以使用encoder_cell代替，即共享权重，但是不共享权重可以得到更佳的性能
decoder_cell = get_multi_rnn_cell(self.config.rnn_size, self.config.num_layers)
output_layer = Dense(self.config.target_vocab_size, kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))

# 训练阶段
# def train():
#     with tf.variable_scope("decode"):
training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=target_embedding_inputs, sequence_length=self.target_sequence_length)
training_decoder = tf.contrib.seq2seq.BasicDecoder(
    decoder_cell,
    training_helper,
    encoder_state,  # # 使用encoder模块的输出状态来初始化states
    # 在输出添加full connected wrapper，映射得到每个target_vocab的score
    output_layer)
training_decoder_output, _, __ = tf.contrib.seq2seq.dynamic_decode(
    training_decoder,
    impute_finished=True,  # 遇到EOS自动停止解码（EOS之后的所有time step的输出为0，输出状态为最后一个有效time step的输出状态）
    maximum_iterations=None)  # 设置最大decoding time steps数量，默认decode until the decoder is fully done，因为训练时会将target序列传入，所以可以为None
self.logits = training_decoder_output.rnn_output

# 测试阶段
# def predict():
#     with tf.variable_scope("decode", reuse=True):
predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
    target_embedding,
    tf.fill([tf.shape(self.source)[0]], self.config.target_letter_to_id['<GO>']),
    self.config.target_letter_to_id['<EOS>'])
predicting_decoder = tf.contrib.seq2seq.BasicDecoder(
    decoder_cell,
    predicting_helper,
    encoder_state,
    output_layer)
predicting_decoder_output, _, __ = tf.contrib.seq2seq.dynamic_decode(
    predicting_decoder,
    impute_finished=True,  # 遇到EOS自动停止解码输出（停止输出，输出状态为最后一个有效time step的输出状态）
    maximum_iterations=tf.round(tf.reduce_max(self.source_sequence_length) * 2))  # 预测时不知道什么时候输出EOS，所以要设置最大time step数量
self.result_ids = predicting_decoder_output.sample_id  # 输出target vocab id
```

最后是模型训练（计算梯度、优化损失函数）阶段。
```
# 3. optimize
masks = tf.sequence_mask(self.target_sequence_length, tf.reduce_max(self.target_sequence_length), dtype=tf.float32)
# logits:[batch_size, 10, 30], targets:[batch_size, 10], masks:[batch_size, 10]
# 让mask的内容不计算损失，如果不做mask，即使impute_finished=True使得EOS之后的输出为0，但是targets的PAD会被误认为是一个正常的翻译结果而计算了损失
self.loss = tf.contrib.seq2seq.sequence_loss(self.logits, self.target, masks)
optimizer = tf.train.AdamOptimizer(self.config.learning_rate)
# 梯度裁剪
gradients = optimizer.compute_gradients(self.loss)
capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]
self.train_op = optimizer.apply_gradients(capped_gradients)
```

注意在训练和预测阶段有不同的形式，因为在预测阶段我们只有source sentence。在预测阶段我们有三种不同的decoding方式，greedy、sampling、beam-search，我们上面代码使用的是greedy策略，即每个time step选择得分最高的输出单词，并作为下一个time step的输入。
![S2S](/img/S2S-03.png)

# 模型训练

代码地址 <a href="https://github.com/qianshuang/seq2seq" target="_blank">https://github.com/qianshuang/seq2seq</a>
运行结果：
```
Configuring model...
Loading data...
Training and evaluating...
Epoch: 1
Iter:      0, Train Loss:    3.4, Val Loss:    3.4, Time: 0:00:02 *
Epoch: 2
Iter:     10, Train Loss:    3.4, Val Loss:    3.4, Time: 0:00:04 *
Epoch: 3
Iter:     20, Train Loss:    3.3, Val Loss:    3.3, Time: 0:00:07 *
Epoch: 4
Iter:     30, Train Loss:    3.1, Val Loss:    3.1, Time: 0:00:10 *
Epoch: 5
Epoch: 6
Iter:     40, Train Loss:    3.0, Val Loss:    3.0, Time: 0:00:12 *
Epoch: 7
Iter:     50, Train Loss:    3.0, Val Loss:    3.0, Time: 0:00:15 *
Epoch: 8
Iter:     60, Train Loss:    2.8, Val Loss:    2.9, Time: 0:00:17 *
......
......
Epoch: 314
Iter:   2510, Train Loss:  0.091, Val Loss:    0.4, Time: 0:07:56 
Epoch: 315
Epoch: 316
Iter:   2520, Train Loss:  0.078, Val Loss:    0.4, Time: 0:07:56 
No optimization for a long time, auto-stopping...
Testing...
原始输入:  common
输出: cmmmoo<EOS>
```

# 社群

- QQ交流群
	![562929489](/img/qq_ewm.png)
- 微信交流群
	![562929489](/img/wx_ewm.png)
- 微信公众号
	![562929489](/img/wxgzh_ewm.png)