---
layout:     post   				    # 使用的布局
title:      58.0 基于深度学习的推荐系统——BST & DSIN			# 标题 
date:       2020-08-03  			# 时间
author:     钱爽 						# 作者
catalog: true 						# 是否归档
tags:								# 标签
    - 推荐系统
---

# BST

BST（Behavior Sequence Transformer），其实就是通过Transformer取代了DIEN中的双层GRU，试图通过Transformer来更好的model用户兴趣随着时间的演化过程。模型架构如下：
![BST](/img/BST-01.png)
模型架构一目了然，除了引入Transformer之外，并没有太多其他创新的地方。唯一需要注意的就是position embedding的计算。位置特征用来刻画用户历史行为序列中的顺序信息，第i个位置的位置特征计算方式为：pos(vi)=t(vt)-t(vi)，其中，t(vt)表示当前候选item，t(vi)表示用户点击商品vi时的时间戳（其实就是按照行为序列的开始时间和结束时间做切片，看当前时间戳落在哪个时间片内）。

# DSIN

DSIN（Deep Session Interest Network），将用户行为序列进一步细粒度化到session维度，因为同一个session内的行为高度同构（兴趣类似），不同session间的behavior异构，DSIN将这种信息也利用了起来，目的是抽取用户在每个session层面的interest，然后捕获session interest之间的用户行为序列关系。DSIN模型架构如下所示：
![DSIN](/img/DSIN-01.png)
下面我们一层层的来看。

## Session Division Layer

即根据特定规则将用户行为序列划分成多个session，一般是session间的时间间隔大于等于30min。相当于输入增加了一个维度变为n x s x t，n为用户数，s为每个用户的session数，t为每个session中的行为数。

## Session Interest Extractor Layer

这一层的目的是寻找session内部的行为之间关系，来进一步提取session interest。即使同个session中的行为是高度同构的，但是还是会有一些随意的一些行为会使得session的兴趣表示变得不准确（比如我想买衣服，看着看着衣服，但不小心误触到其他推送内容，但那和我的真实想去点击的东西无关）。所以为了对同一session中多个行为的关系进行建模和减轻那些不相关行为的影响，注意在每个session内，使用Transformer来抽取每个session的兴趣特征。

不过在输入Transformer前还做了bias encoding，其实就是对Transformer中的position encoding做出了优化，命名为bias encoding（BE）。因为我们引入了session，所以需要考虑三层位置：每个用户有k个session，每个session又有t个behavior，每个behavior又有c个元素（向量维度）。所以bias encoding表达式为：
![DSIN](/img/DSIN-02.png)
经过Transformer后的结果再在session维度做avg pooling得到每个session的interest向量表示。

## Session Interest Interacting Layer

在经过Session Interest Extractor Layer提取出每个session的interest后，很自然就会想要去捕获不同session之间的交互关系，这里使用Bi-LSTM来做这件事。

## Session Interest Activating Layer

Item Profile（Item field）为商家ID、品牌ID的embedding，我们把跟商品i有关的特征考虑进来，跟用户的每一个session interest计算Attention score，最终通过softmax得到Attention weight。

对于跨session的interest，采用同样的做法。