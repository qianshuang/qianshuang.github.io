---
layout:     post   				    # 使用的布局
title:      24.0 循环神经网络 				# 标题 
date:       2018-08-27 				# 时间
author:     子颢 						# 作者
catalog: true 						# 是否归档
tags:								#标签
    - 深度学习
    - RNN
    - 循环神经网络
    - LSTM
    - GRU
---

# 算法原理

RNN（Recurrent Neural Networks，循环神经网络），他的最大特点是时序性，在每一个time step，神经元接收当前的输入Xt，以及上一个time step的神经元的输出状态St-1一起产生当前的输出Yt，以及当前的状态St（St默认情况下和输出Yt相同）。
![RNN](/img/RNN-01.png)
下面举个例子来说明，假设每个time step的状态是二维，并且输入输出都是一维，循环体中的权重矩阵W1=[[0.1,0.2],[0.3,0.4],[0.5,0.6]]（两列代表有两个神经元），b1=[1.0,2.0]，输出的全连接层权重W2=[1.0,2.0]，b2=[0.1]，那么整个计算过程为：
![RNN](/img/RNN-03.png)
可以看到，两个神经元中，无论是循环体中的权重矩阵，还是输出的全连接层权重矩阵也都是共享的。

RNN的表现形式有很多种：
![RNN](/img/RNN-02.png)
左上角为seq to seq：输入是一个序列，输出是一个序列。可以用来做股票预测，即输入过去N天的股票价格，输出明天到后N-1天的股票价格。<br>
右上角为seq to vector：输入一个序列，但是只保留最后一个输出。可以用来做文本分类，输入文本序列，输出类别标签。<br>
左下角为vector to seq：只在第一个time step输入真实数据，后面的time step输入0，输出为一个序列。比如看图说话，输入为一张图片，输出图片标题。<br>
右下角为delayed seq to seq：又叫做Encoder–Decoder，典型的应用场景就是机器翻译。输入为一种语言的句子，encode模块先将输入转换为一个向量表示，decode模块再将该向量表示解码为另一种语言的句子。这种方式的效果比及时翻译（直接采用seq to seq）要好，因为后面的输入可能会影响先前的翻译结果，所以需要等到接收到整个输入句子再做翻译。





# 模型训练
代码地址 <a href="https://github.com/qianshuang/dl-exp" target="_blank">https://github.com/qianshuang/dl-exp</a>
```

```
运行结果：
```

```

# 社群

- QQ交流群
	![562929489](/img/qq_ewm.png)
- 微信交流群
	![562929489](/img/wx_ewm.png)
- 微信公众号
	![562929489](/img/wxgzh_ewm.png)