---
layout:     post   				    # 使用的布局
title:      01.0 K近邻算法 				# 标题 
# subtitle:   Hello World, Hello Blog # 副标题
date:       2018-07-16 				# 时间
author:     子颢 						# 作者
# header-img: img/post-bg-2015.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 机器学习
---

# 视频地址

在线观看 <a href="" target="_blank"></a>

百度网盘 <a href="https://pan.baidu.com/s/1jPlLafsc-Ed5_6NJvD2y_g" target="_blank">https://pan.baidu.com/s/1jPlLafsc-Ed5_6NJvD2y_g</a>

# 算法原理

k近邻（k-Nearest Neighbor，kNN），应该是最简单的传统机器学习模型，给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例中的大多数属于哪个类别，就把该输入实例划分到这个类别。
![kNN算法模型图](/img/kNN-01.png)

k近邻算法没有显示的训练过程，在“训练阶段”仅仅是把样本保存起来，训练时间开销为零，待收到测试样本后在进行计算处理。

这个k实际上是一个超参数，k值的选择会对k近邻法的结果产生重大影响。如果选择较小的k值，意味着只有与输入实例较近的（相似的）训练实例才会对预测结果起作用，预测结果会对近邻的实例点非常敏感，如果近邻的实例点恰巧是噪声点，预测就会出错；如果选择较大的k值，就意味着与输入实例较远的（不相似的）训练实例也会对预测起作用，这样预测也会出错。在实际应用中，k值一般取一个比较小的数值，并且通常采用交叉验证法来选取最优的k值。如上图的k=5。

# 模型训练

代码地址 <a href="" target="_blank"></a>
代码解析请观看视频讲解。

# 社群

- QQ交流群
	![562929489](/img/qq_ewm.png)
- 微信交流群
	![562929489](/img/wx_ewm.png)
- 微信公众号
	![562929489](/img/wxgzh_ewm.png)