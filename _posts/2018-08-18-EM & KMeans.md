---
layout:     post   				    # 使用的布局
title:      15.0 期望最大化 & k-means聚类		# 标题 
date:       2018-08-18 				# 时间
author:     子颢 						# 作者
catalog: true 						# 是否归档
tags:								#标签
    - 机器学习
    - EM
    - 期望最大化
    - k-means聚类
    - k均值聚类
---

# 算法原理

## EM

期望最大化（Expectation Maximizition，EM），在前面的讨论中，我们一直假设训练样本所有属性变量的值都已被观测到，即训练样本是完整的，但是在现实应用中，往往会遇到不完整的训练样本，即我们知道有一个属性变量对模型至关重要，但是无法获得这个属性变量的值。在这种存在未观测变量（隐变量）的情形下，是否仍能对模型参数进行估计呢？
![EM](/img/EM-01.png)
![EM](/img/EM-02.png)
EM算法是常用的在含有隐变量的情况下，估计模型参数的利器（训练完成后最终还可以求得隐变量的参数值）。其基本思想是：
1. 首先初始化模型参数θ；
2. 然后根据训练数据和当前的模型参数θ推断出最优隐变量Z的值（即求Z的期望，E步）；
3. 基于训练数据和Z的最优期望值对参数θ做极大似然估计（M步）；
4. 迭代的将2、3交替进行，直到收敛到局部最优解。

EM算法对初值敏感，对于不同的初始值，可能会导致不同的结果，并且它对于“躁声”和孤立点数据也是敏感的，少量的该类数据能够对模型产生极大的影响。

实际上若隐变量的情况已知，那么我们可以直接根据最大似然估计和随机梯度下降，解出在每一种隐变量情况下的参数和最大似然估计值，然后选择似然估计最大情况下的模型和隐变量。

举例：考虑数据集D是一个实例集合，它由k个不同的正态分布的混合分布所生成（如下图k等于2），现在通过EM算法训练求解此混合高斯模型？
![EM](/img/EM-06.png)
这里可把每个实例的完整描述看作是一个三元组<xi,zi1, zi2>，其中xi是第i个实例的观测值，zi1和zi2表示两个正态分布中哪个被用于产生值xi，确切地讲，zij在xi由第j个正态分布产生时值为1，否则为0，可见zi1和zi2是隐藏变量。算法步骤如下：
1. 首先初始化模型参数h=<μ1,μ2>（两个高斯模型的均值）；
2. 根据当前的模型，计算每个隐藏变量zij的期望值E[zij]（最优值，可由将当前值<μ1,μ2>和所有样本点代入到下式中计算得到）；
![EM](/img/EM-07.png)
3. 使用上步中得到的E[zij]来计算新的极大似然参数h´=<μ1´,μ2´>（其实是对μj的加权样本均值）；
![EM](/img/EM-08.png)
4. 迭代的将2、3交替进行，直到收敛到局部最优解。

## k-means

聚类是一种最典型的无监督学习任务，而k-means聚类是所有聚类算法的典型。
![EM](/img/EM-03.png)
![EM](/img/EM-04.png)
![EM](/img/EM-05.png)
直接最小化式(9.24)并不容易，找到它的最优解需要考察样本集D所有可能的簇划分，这是一个NP难问题，因此k-means算法采用了贪心策略，通过迭代优化来近似求解式(9.24)，算法流程如下：
1. 从D中随机选择k个样本作为初始质心；（k为超参数）
2. 将所有样本点归类到距离质心最近的簇中；
3. 重新计算簇的质心作为新的质心；
4. 迭代的将2、3交替进行，直到收敛到局部最优解。

可以发现，k-means算法和EM算法原型惊人的相似，其实k-means就是EM算法的一种特殊实现。我们的模型参数是簇的质心，根据簇的质心将所有样本点归类到距离质心最近的簇中（E），重新计算簇的质心作为新的质心使损失函数式(9.24)最小化（M）。

# 模型训练

代码地址 <a href="https://github.com/qianshuang/ml-exp" target="_blank">https://github.com/qianshuang/ml-exp</a>

```
def train():
    print("start training...")
    # 处理训练数据
    # train_feature, train_target = process_file(train_dir, word_to_id, cat_to_id)  # 词频特征
    train_data = process_maxent_file(train_dir, word_to_id, cat_to_id)  # 最大熵词频特征
    # train_feature, train_target = process_tfidf_file(train_dir, word_to_id, cat_to_id)  # TF-IDF特征
    # 模型训练
    # model.fit(train_feature, train_target)
    model.train(train_data)


def test():
    print("start testing...")
    # 处理测试数据
    # test_feature, test_target = process_file(test_dir, word_to_id, cat_to_id)
    test_data = process_maxent_file(test_dir, word_to_id, cat_to_id)  # 最大熵词频特征
    # test_feature, test_target = process_tfidf_file(test_dir, word_to_id, cat_to_id)  # 不能直接这样处理，应该取训练集的IDF值
    # test_predict = model.predict(test_feature)  # 返回预测类别
    # test_predict_proba = model.predict_proba(test_feature)    # 返回属于各个类别的概率
    # test_predict = np.argmax(test_predict_proba, 1)  # 返回概率最大的类别标签

    # MaxEnt测试
    wordid_freq_jsons = []
    test_target = []
    for i in range(len(test_data)):
        wordid_freq, label_id = test_data[i]
        test_target.append(label_id)
        wordid_freq_jsons.append(json.dumps(wordid_freq))

    test_predict = model.prob_classify_many(wordid_freq_jsons)

    # accuracy
    true_false = (test_predict == test_target)
    accuracy = np.count_nonzero(true_false) / float(len(test_target))
    print()
    print("accuracy is %f" % accuracy)

    # precision    recall  f1-score
    print()
    print(metrics.classification_report(test_target, test_predict, target_names=categories))

    # 混淆矩阵
    print("Confusion Matrix...")
    print(metrics.confusion_matrix(test_target, test_predict))


if not os.path.exists(vocab_dir):
    # 构建词典表
    build_vocab(train_dir, vocab_dir)

categories, cat_to_id = read_category()
words, word_to_id = read_vocab(vocab_dir)

# kNN
# model = neighbors.KNeighborsClassifier()
# decision tree
# model = tree.DecisionTreeClassifier()
# random forest
# model = ensemble.RandomForestClassifier(n_estimators=10)  # n_estimators为基决策树的数量，一般越大效果越好直至趋于收敛
# AdaBoost
# model = ensemble.AdaBoostClassifier(learning_rate=1.0)  # learning_rate的作用是收缩基学习器的权重贡献值
# GBDT
# model = ensemble.GradientBoostingClassifier(n_estimators=10)
# xgboost
# model = xgboost.XGBClassifier(n_estimators=10)
# Naive Bayes
# model = naive_bayes.MultinomialNB()
# logistic regression
# model = linear_model.LogisticRegression()   # ovr
# model = linear_model.LogisticRegression(multi_class="multinomial", solver="lbfgs")  # softmax回归
# SVM
# model = svm.LinearSVC()  # 线性，无概率结果
# model = svm.SVC(probability=True)  # 核函数，训练慢
# MaxEnt
model = nltk.classify.MaxentClassifier


train()
test()
```
运行结果：
```
read_category...
read_vocab...
start training...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -2.30259        0.100
·············
·············
```

# 社群

- QQ交流群
	![562929489](/img/qq_ewm.png)
- 微信交流群
	![562929489](/img/wx_ewm.png)
- 微信公众号
	![562929489](/img/wxgzh_ewm.png)