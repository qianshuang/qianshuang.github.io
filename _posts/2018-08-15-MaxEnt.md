---
layout:     post   				    # 使用的布局
title:      12.0 最大熵模型 				# 标题 
# subtitle:   Hello World, Hello Blog # 副标题
date:       2018-08-15 				# 时间
author:     子颢 						# 作者
# header-img: img/post-bg-2015.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 机器学习
    - MaxEnt
    - 最大熵
---

# 算法原理

最大熵模型（Max Entroy，MaxEnt），在所有满足约束条件的模型集合中，熵最大的模型是最好的模型。论及投资，人们常说不要把鸡蛋放在一个篮子里，这样分散投资可以降低风险，进而达到收益最大化，因为越平均（越分散，能保留全部的不确定性），熵越大。
![最大熵](/img/ME-01.png)

下面我们引入特征函数的概念，特征函数是特征的函数式表示，跟特征一样，需要通过特征工程得到。
![最大熵](/img/ME-02.png)
![最大熵](/img/ME-03.png)
![最大熵](/img/ME-04.png)
其实可以证明求熵的最大值等价于最大熵模型的极大似然估计。
求解方式我们应该已经很熟悉了，使用拉格朗日乘子法，然后利用偏导数为0，求得：
![最大熵](/img/ME-05.png)
通过观察可以发现，最大熵模型和逻辑回归模型的softmax版本有完全一模一样的形式，所以完全可以在给定训练数据的条件下，对模型进行极大似然估计，然后通过随机梯度下降来进行训练。实际中对于最大熵模型的训练，通常采用改进的迭代尺度算法（IIS）和拟牛顿法。

下面通过一个实际的例子来让大家直观的感受一下最大熵模型的训练过程：
对于我们的文本分类问题，假设我们有三个类别：C1、C2、C3，我们现在以词频作为特征函数，那么可以构造下面的一张特征函数表如下，x代表词频特征函数，λ代表特征函数权重。

|  | word1 | word2 | word3 | ··· | wordn |
| ------ | ------ | ------ | ------ | ------ | ------ |
| C1 | λ11*x1 | λ12*x2 | λ13*x3 | ··· | λ1n*xn |
| C2 | λ21*x1 | λ22*x2 | λ23*x3 | ··· | λ2n*xn |
| C3 | λ31*x1 | λ32*x2 | λ33*x3 | ··· | λ3n*xn |

假设某篇文章由word1、word2、word3组成，求这篇文章属于各个类别的概率？如何训练模型求出模型参数λ？

# 模型训练

代码地址 <a href="https://github.com/qianshuang/ml-exp" target="_blank">https://github.com/qianshuang/ml-exp</a>

```
def train():
    print("start training...")
    # 处理训练数据
    # train_feature, train_target = process_file(train_dir, word_to_id, cat_to_id)  # 词频特征
    train_data = process_maxent_file(train_dir, word_to_id, cat_to_id)  # 最大熵词频特征
    # train_feature, train_target = process_tfidf_file(train_dir, word_to_id, cat_to_id)  # TF-IDF特征
    # 模型训练
    # model.fit(train_feature, train_target)
    model.train(train_data)


def test():
    print("start testing...")
    # 处理测试数据
    # test_feature, test_target = process_file(test_dir, word_to_id, cat_to_id)
    test_data = process_maxent_file(test_dir, word_to_id, cat_to_id)  # 最大熵词频特征
    # test_feature, test_target = process_tfidf_file(test_dir, word_to_id, cat_to_id)  # 不能直接这样处理，应该取训练集的IDF值
    # test_predict = model.predict(test_feature)  # 返回预测类别
    # test_predict_proba = model.predict_proba(test_feature)    # 返回属于各个类别的概率
    # test_predict = np.argmax(test_predict_proba, 1)  # 返回概率最大的类别标签

    # MaxEnt测试
    wordid_freq_jsons = []
    test_target = []
    for i in range(len(test_data)):
        wordid_freq, label_id = test_data[i]
        test_target.append(label_id)
        wordid_freq_jsons.append(json.dumps(wordid_freq))

    test_predict = model.prob_classify_many(wordid_freq_jsons)

    # accuracy
    true_false = (test_predict == test_target)
    accuracy = np.count_nonzero(true_false) / float(len(test_target))
    print()
    print("accuracy is %f" % accuracy)

    # precision    recall  f1-score
    print()
    print(metrics.classification_report(test_target, test_predict, target_names=categories))

    # 混淆矩阵
    print("Confusion Matrix...")
    print(metrics.confusion_matrix(test_target, test_predict))


if not os.path.exists(vocab_dir):
    # 构建词典表
    build_vocab(train_dir, vocab_dir)

categories, cat_to_id = read_category()
words, word_to_id = read_vocab(vocab_dir)

# kNN
# model = neighbors.KNeighborsClassifier()
# decision tree
# model = tree.DecisionTreeClassifier()
# random forest
# model = ensemble.RandomForestClassifier(n_estimators=10)  # n_estimators为基决策树的数量，一般越大效果越好直至趋于收敛
# AdaBoost
# model = ensemble.AdaBoostClassifier(learning_rate=1.0)  # learning_rate的作用是收缩基学习器的权重贡献值
# GBDT
# model = ensemble.GradientBoostingClassifier(n_estimators=10)
# xgboost
# model = xgboost.XGBClassifier(n_estimators=10)
# Naive Bayes
# model = naive_bayes.MultinomialNB()
# logistic regression
# model = linear_model.LogisticRegression()   # ovr
# model = linear_model.LogisticRegression(multi_class="multinomial", solver="lbfgs")  # softmax回归
# SVM
# model = svm.LinearSVC()  # 线性，无概率结果
# model = svm.SVC(probability=True)  # 核函数，训练慢
# MaxEnt
model = nltk.classify.MaxentClassifier


train()
test()
```
运行结果：
```
read_category...
read_vocab...
start training...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -2.30259        0.100
·············
·············
```

# 社群

- QQ交流群
	![562929489](/img/qq_ewm.png)
- 微信交流群
	![562929489](/img/wx_ewm.png)
- 微信公众号
	![562929489](/img/wxgzh_ewm.png)