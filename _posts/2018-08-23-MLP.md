---
layout:     post   				    # 使用的布局
title:      20.0 全连接神经网络 				# 标题 
date:       2018-08-23 				# 时间
author:     子颢 						# 作者
catalog: true 						# 是否归档
tags:								#标签
    - 深度学习
    - MLP
    - Multi-Layer Perceptron
    - 全连接神经网络
    - full-connected neural networks
    - DNN
    - deep neural network
    - 多层前馈神经网络
---

# 算法原理

在生物神经网络中，每个神经元与其他神经元相连，当它兴奋时，就会向相连的神经元发送化学物质，从而改变这些神经元内的电位。如果某神经元的电位超过了一个阈值，那么它就会被激活，从而兴奋起来，继续向其他神经元发送化学物质，从而将信号逐层传递下去。
![MLP](/img/MLP-01.png)
如上所示，神经元接收到来自n个其他神经元传递过来的输入信号，并且每个输入信号带有一定的权重，神经元接收到的总输入值将与神经元的阈值比较，然后通过激活函数处理以产生神经元的输出（决定是否被激活）。
![MLP](/img/MLP-02.png)
把许多个这样的神经元按照一定的层次结构连接起来，就得到了神经网络。

之所以又把DNN叫多层感知机Multi-Layer Perceptron，是因为我们的逻辑回归实际上就是一个一层的神经网络，只有输入层和输出层，没有隐藏层，这就是一个感知机，所以感知机本质上还是一个对数线性模型，无法处理非线性问题，但是多层感知机（含有1到多个隐藏层的神经网络）可以，可以证明，只需一个包含足够多神经元的隐层，MLP就能以任意精度逼近任意复杂度的连续函数。
![MLP](/img/MLP-03.png)

神经网络（DNN、CNN、RNN等）的训练都是通过反向传播（backpropagation）完成的，反向传播是梯度下降算法的一种特殊实现，是神经网络专用的参数更新算法，参数的更新过程也是通过梯度下降的公式推到计算而来的。（推导过程详见：《机器学习》周志华第5.3节）：
![MLP](/img/logistic_regression-05.png)
反向传播最终是根据偏误差反向传递。假设我们取激活函数为Sigmoid，Oj代表每一个神经元（隐藏层和输出层）的输出值，Oi为每个神经元的实际值，那么每个神经元的偏误差为：
![MLP](/img/MLP-04.png)
下面通过一个实际例子来说明反向传播的具体过程：







激活函数
![MLP](/img/MLP-05.png)


# 模型训练

## sklearn版
代码地址 <a href="https://github.com/qianshuang/ml-exp" target="_blank">https://github.com/qianshuang/ml-exp</a>

## tensorflow版
代码地址 <a href="https://github.com/qianshuang/dl-exp" target="_blank">https://github.com/qianshuang/dl-exp</a>

# 社群

- QQ交流群
	![562929489](/img/qq_ewm.png)
- 微信交流群
	![562929489](/img/wx_ewm.png)
- 微信公众号
	![562929489](/img/wxgzh_ewm.png)